{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e267cf-d788-4036-9eee-8a1e084e3ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall metrics:\n",
      "     model  accuracy      f1  roc_auc  sex_dp_gap  sex_eopp_gap  race_dp_gap  \\\n",
      "0  logreg    0.8453  0.6521   0.9056      0.1790        0.1369       0.1932   \n",
      "1      rf    0.8409  0.6575   0.8896      0.1845        0.1059       0.1515   \n",
      "\n",
      "   race_eopp_gap  \n",
      "0         0.4449  \n",
      "1         0.2278  \n",
      "\n",
      "Saved: C:\\Users\\hana1\\Documents\\iva-bias-project\\results\\metrics\\adult_overall_metrics.csv\n",
      "\n",
      "Sample group metrics:\n",
      "      model attribute               group     n     p_pos  accuracy       tpr\n",
      "0   logreg       sex              Female  3681  0.076066  0.916599  0.470199\n",
      "1   logreg       sex                Male  7625  0.255082  0.810885  0.607067\n",
      "2   logreg      race  Amer-Indian-Eskimo   106  0.084906  0.924528  0.538462\n",
      "3   logreg      race  Asian-Pac-Islander   320  0.262500  0.853125  0.717647\n",
      "4   logreg      race               Black  1044  0.091954  0.899425  0.467153\n",
      "5   logreg      race               Other   101  0.069307  0.881188  0.272727\n",
      "6   logreg      race               White  9735  0.208423  0.838007  0.588419\n",
      "7       rf       sex              Female  3681  0.092366  0.914425  0.527594\n",
      "8       rf       sex                Male  7625  0.276852  0.805377  0.633461\n",
      "9       rf      race  Amer-Indian-Eskimo   106  0.150943  0.877358  0.615385\n",
      "10      rf      race  Asian-Pac-Islander   320  0.240625  0.856250  0.682353\n",
      "11      rf      race               Black  1044  0.100575  0.913793  0.554745\n",
      "Saved: C:\\Users\\hana1\\Documents\\iva-bias-project\\results\\metrics\\adult_group_metrics.csv\n",
      "\n",
      "Saved best model: logreg  | score=0.9056  -> C:\\Users\\hana1\\Documents\\iva-bias-project\\models\\adult\\best_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# Adult Income â€” Baseline training + fairness metrics\n",
    "# Input :  data/adult_model.csv  (features + binary target 'label')\n",
    "# Output:  results/metrics/adult_overall_metrics.csv\n",
    "#          results/metrics/adult_group_metrics.csv\n",
    "#          models/adult/best_pipeline.joblib\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "project_root = Path.cwd().resolve().parent  # run from notebooks/\n",
    "data_dir = project_root / \"data\"\n",
    "results_dir = project_root / \"results\" / \"metrics\"\n",
    "models_dir = project_root / \"models\" / \"adult\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(data_dir / \"adult_model.csv\")\n",
    "\n",
    "# Target and features\n",
    "y = df[\"label\"].astype(int).values\n",
    "X = df.drop(columns=[\"label\"]).copy()\n",
    "\n",
    "# Sensitive attributes (if present)\n",
    "sensitive_cols = [c for c in [\"sex\", \"race\"] if c in X.columns]\n",
    "sensitive_df = X[sensitive_cols].copy() if sensitive_cols else pd.DataFrame(index=X.index)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive_df, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Columns by type\n",
    "num_cols = [c for c in X_train.columns if np.issubdtype(X_train[c].dtype, np.number)]\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "def eval_fairness(model_name, pipe, X_te, y_te, sens_te):\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_te)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_te, y_proba)\n",
    "    except Exception:\n",
    "        roc_auc = np.nan\n",
    "\n",
    "    overall = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "        \"f1\": f1_score(y_te, y_pred),\n",
    "        \"roc_auc\": roc_auc,\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    if not sens_te.empty:\n",
    "        def tpr(y_true, y_hat):\n",
    "            pos = (y_true == 1).sum()\n",
    "            return ((y_true == 1) & (y_hat == 1)).sum() / pos if pos > 0 else np.nan\n",
    "\n",
    "        for attr in sens_te.columns:\n",
    "            groups = sens_te[attr].astype(str).unique()\n",
    "            per_group = []\n",
    "            for g in sorted(groups):\n",
    "                mask = sens_te[attr].astype(str) == g\n",
    "                yp_g = y_pred[mask]\n",
    "                yt_g = y_te[mask]\n",
    "                p_pos = (yp_g == 1).mean() if len(yp_g) else np.nan\n",
    "                acc = accuracy_score(yt_g, yp_g) if len(yt_g) else np.nan\n",
    "                tpr_g = tpr(yt_g, yp_g) if len(yt_g) else np.nan\n",
    "                per_group.append({\"p_pos\": p_pos, \"tpr\": tpr_g})\n",
    "\n",
    "                rows.append({\n",
    "                    \"model\": model_name, \"attribute\": attr, \"group\": g,\n",
    "                    \"n\": int(mask.sum()), \"p_pos\": p_pos, \"accuracy\": acc, \"tpr\": tpr_g\n",
    "                })\n",
    "\n",
    "            dp_gap = np.nanmax([r[\"p_pos\"] for r in per_group]) - np.nanmin([r[\"p_pos\"] for r in per_group])\n",
    "            eopp_gap = np.nanmax([r[\"tpr\"] for r in per_group]) - np.nanmin([r[\"tpr\"] for r in per_group])\n",
    "            overall[f\"{attr}_dp_gap\"] = dp_gap\n",
    "            overall[f\"{attr}_eopp_gap\"] = eopp_gap\n",
    "\n",
    "    return overall, rows\n",
    "\n",
    "overall_metrics, group_metrics = [], []\n",
    "best_name, best_score, best_pipe = None, -np.inf, None\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    overall, rows = eval_fairness(name, pipe, X_test, y_test, sens_test)\n",
    "    overall_metrics.append(overall)\n",
    "    group_metrics.extend(rows)\n",
    "\n",
    "    key = \"roc_auc\" if not np.isnan(overall[\"roc_auc\"]) else \"f1\"\n",
    "    score = overall[key]\n",
    "    if score is not None and (not np.isnan(score)) and score > best_score:\n",
    "        best_name, best_score, best_pipe = name, score, pipe\n",
    "\n",
    "# Save metrics\n",
    "overall_df = pd.DataFrame(overall_metrics)\n",
    "group_df = pd.DataFrame(group_metrics) if group_metrics else pd.DataFrame(columns=[\n",
    "    \"model\",\"attribute\",\"group\",\"n\",\"p_pos\",\"accuracy\",\"tpr\"\n",
    "])\n",
    "\n",
    "overall_path = results_dir / \"adult_overall_metrics.csv\"\n",
    "group_path = results_dir / \"adult_group_metrics.csv\"\n",
    "overall_df.to_csv(overall_path, index=False)\n",
    "group_df.to_csv(group_path, index=False)\n",
    "\n",
    "print(\"Overall metrics:\\n\", overall_df.round(4))\n",
    "print(\"\\nSaved:\", overall_path)\n",
    "if not group_df.empty:\n",
    "    print(\"\\nSample group metrics:\\n\", group_df.head(12))\n",
    "    print(\"Saved:\", group_path)\n",
    "\n",
    "# Save best model\n",
    "if best_pipe is not None:\n",
    "    out_model = models_dir / \"best_pipeline.joblib\"\n",
    "    joblib.dump(best_pipe, out_model)\n",
    "    print(f\"\\nSaved best model: {best_name}  | score={best_score:.4f}  -> {out_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634bb3-b95e-4403-b6c7-c84ed1cbd020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
